from numpy.linalg import inv as np_inv
from numpy.linalg import svd, det, cholesky
# from numpy.linalg import slogdet as np_slogdet
from numpy import array, trace, log, diag
from numpy import sqrt as np_sqrt
from scipy.linalg import sqrtm
from scipy.optimize import bisect, brenth, minimize_scalar, LinearConstraint, minimize
import numpy as np
import sys
from fpylll import *
from fpylll.algorithms.bkz2 import BKZReduction

ROUNDING_FACTOR = 2**64

def round_matrix_to_rational(M):
    A = matrix(ZZ, (ROUNDING_FACTOR * matrix(M)).apply_map(round))
    return matrix(QQ, A / ROUNDING_FACTOR)

def projection_matrix(A):
    """
    Construct the projection matrix orthogonally to Span(V)
    """
    S = A * A.T
    return A.T * S.inverse() * A

def square_root_inverse_degen(S, B=None, assume_full_rank=False):
    """ Compute the determinant of a symmetric matrix
    sigma (m x m) restricted to the span of the full-rank
    rectangular (k x m, k <= m) matrix V
    """
    
    if assume_full_rank:
        P = identity_matrix(S.ncols())

    elif not assume_full_rank and B is None:
        # Get an orthogonal basis for the Span of B
        V = S.echelon_form()
        V = V[:V.rank()]
        P = projection_matrix(V)

    else:
        P = projection_matrix(B)

    # make S non-degenerated by adding the complement of span(B)
    C = identity_matrix(S.ncols()) - P
    # Take matrix sqrt via SVD, then inverse
    # S = adjust_eigs(S)
    
    u, s, vh = svd(array(S + C, dtype=float))
    L_inv = np_inv(vh) @ np_inv(np_sqrt(diag(s))) @ np_inv(u)
    # L_inv = np_inv(sqrtm(array(S + C, dtype=float)))
    
    L_inv = np_inv(cholesky(array(S + C, dtype=float))).T
    L_inv = round_matrix_to_rational(L_inv)
    L = L_inv.inverse()


    # scipy outputs complex numbers, even for real valued matrices. Cast to real before rational.
    #L = round_matrix_to_rational(u @ np_sqrt(diag(s)) @ vh)
  
    return L, L_inv

class MIE:
    def __init__(self, S, mu):
        self.S = S
        self.mu = mu

    def dim(self):
        return len(self.mu)
    
    # NOTE: in the toolkit everything is done with rows instead of columns
    def integrate_parallel_cuts_hint(self, direction, a, b):
        # the meaning of the signs here is kind of superfluous, this is just
        # to determine where everything is relative to the center of the
        # ellipsoid
        # 
        # a and b are distances from the center of the ellipsoid to the
        # two hyperplanes in the hint, along the direction of direction

        # there are problems if the direction is not in the column space
        # right now just error out
        if not direction in self.S.column_space():
            #return InvalidHint("direction not in column space of Sigma")
            print("ur dum lol")

        (sqrt_mat, sqrt_inv_mat) = square_root_inverse_degen(self.S)

        # Step 1, subtract
        direction -= self.mu

        # this is to accommodate for step 2 in our drawing
        a_scaled = sqrt_mat * (direction / norm(direction)) * a
        b_scaled = sqrt_mat * (direction / norm(direction)) * b

        print("all scaled")
        
        # this is to apply a Householder transformation
        e = zero_vector(self.dim())
        print("after zero")
        e[0] = 1
        print("make it e1")
        refl = (e - direction) / 2
        
        # this step might cause issues, ellipsoids are over the reals,
        # but when we multiply it by the lattice, this thing has to be rational
        # a solution is to round to the nearest rational with some precision,
        # but this can cause problems when done over and over again
        # for a single hint, this _should_ be fine, still keep note of this
        # though
        # since we are rotating and rotating back, some things should cancel out
        # intuitively, but we might have to figure this out later
        # if estimating, we need not worry about this
        G = AffineGroup(self.dim(), RR) # could be rationals, check back with this later
        print("about to make relf_mat")
        refl_mat = G.reflection(refl)
        print("got refl_mat")

        a_scaled_rot = refl_mat.A() * a_scaled + refl_mat.b()
        b_scaled_rot = refl_mat.A() * b_scaled + refl_mat.b()

        # now we have a unit ball with the first coordinate aligned, make sure
        # a and b are witihin this ball (since they are scalar multiples of
        # e_1, we only have to check the first coordinate)

        alpha = a_scaled_rot[0]
        beta = b_scaled_rot[0]

        alpha, beta = min(alpha, beta), max(alpha, beta)

        print(f"{alpha}, {beta}")

        # in the future we might want to make it so that we assume the extreme
        # hyperplane is the tangent plane of the hypersphere
        if abs(alpha) > 1 or abs(beta) > 1:
            #raise InvalidHint("alpha or beta is too big to be useful")
            print("super dum")

        # this is a and b in the paper, changed names to avoid confusion
        matrix_first = 0
        matrix_rest = 0
        tau = 0
        n = self.dim()
        left_condition = 4 * n * (1 - alpha) * (1 + alpha)
        right_condition = (n + 1) * (n + 1) * (beta - alpha) * (beta + alpha)

        print("before mess")
        if alpha == -beta:
            print("cond 1")
            tau = 0
            matrix_first = beta
            matrix_rest = 1
        elif left_condition < right_condition:
            print("cond 2")
            tau = 0.5 * (alpha + sqrt(alpha * alpha + left_condition / pow(n + 1, 2)))
            matrix_first = tau - alpha
            matrix_rest = sqrt(matrix_first * (matrix_first + n * tau))
        else: # (left_condition >= right_condition)
            print("cond 3")
            denom = 2 * (sqrt((1 - alpha) * (1 + alpha)) - sqrt((1 - beta) * (1 + beta)))
            tau = 0.5 * (beta + alpha)
            matrix_first = 0.5 * (beta - alpha)
            matrix_rest = sqrt(matrix_first ** 2 + pow((beta ** 2 - alpha ** 2) / denom, 2))

        print(f"{tau}, {matrix_first}, {matrix_rest}")
        print("after mess")
            

        # this is to build up the diagonal matrix as in the paper
        print("zero_vector")
        z = zero_vector(RR, n)
        print("matrix_first")
        z[0] = matrix_first
        print("about to enter for loop")
        for ind in range(1, n):
            z[ind] = matrix_rest
        A = diagonal_matrix(z)
        c = zero_vector(RR, n)
        c[0] = tau

        print(f"before A: {A}, c: {c}")
        
        # this can probably be made better with matrix mulitplication
        # and/or matrix augmentation, but we can't figure it out right now
        # also rotate c here
        # might be able to apply either the matrix, or its transpose at the
        # worst because this is an orthonormal matrix
        for ind in range(self.dim()):
            v = vector(A[:,ind])
            reflected = refl_mat.A() * v + refl_mat.b()
            A[:,ind] = reflected

        print(f"after A: {A}, c: {c}")
        # transform it back and mutate the starting matrix
        # apply sqrt_inv to c
        # Dana mentioned that we might want self.S to be of the form
        # inv_sqrt_mat * S * sqrt_mat
        # because of the properties that we have
        # xSx^T <= 1 (perhaps in order to mirror this property we would have
        #    (x * sqrt_inv_mat) * S * (x * sqrt_inv_mat)^T
        # <=> x * (sqrt_inv_mat * S * inv_mat) * x^T
        self.S = sqrt_inv_mat * A
        c = refl_mat.A() * c + refl_mat.b()
        c = sqrt_inv_mat * c
        self.mu += c
        
